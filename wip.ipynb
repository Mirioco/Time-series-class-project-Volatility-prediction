{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca97c82a-db57-4a78-a74b-a46dde1cf3ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5ee2682-311a-426c-9891-24097a610316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from arch import arch_model\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a7c6c-c30c-4904-b0be-bcdce45ef593",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f344a7-e02e-46ea-92b3-c3fffc90e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp500_ticker_list():\n",
    "    \"\"\"\n",
    "    Returns a list with all SP500 tickers\n",
    "    \"\"\"\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "    tables = pd.read_html(url)\n",
    "    table = tables[0]\n",
    "    ticker_list = table['Symbol']\n",
    "    return ticker_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d289221-e4fa-4e3a-bc07-6830638f05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_ticker(ticker_list, s=5):\n",
    "    sample = ticker_list.sample(s).to_list()\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c159ed3d-6d6b-4898-8eb7-21a63ea8033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_close(ticker_list, start, end, interval):\n",
    "    \"\"\"\n",
    "    Returns the adjusted close for a unique ticker as string or a list of tickers.\n",
    "    Format of dates: 'yyyy-mm-dd'\n",
    "    Possible intervals: '1d', '5d', '1mo' \n",
    "    or intraday measures but limited to max a week's worth: '1m', '2m', '5m', '15m', '30m'\n",
    "    \"\"\"\n",
    "    full_df = yf.download(ticker_list, start=start, end=end, interval=interval)\n",
    "    adj_close_df = full_df['Adj Close']\n",
    "    return adj_close_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c710b41a-4774-48aa-a632-bd1367ad42d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_close_df(s=5, start='2020-01-01', end='2021-12-31', interval='1d'):\n",
    "    \"\"\"\n",
    "    Returns the returns df and adj close df for a unique ticker as string or a list of tickers.\n",
    "    n = sample size\n",
    "    Format of dates: 'yyyy-mm-dd'\n",
    "    Possible intervals: '1d', '5d', '1mo' \n",
    "    or intraday measures but limited to max a week's worth: '1m', '2m', '5m', '15m', '30m'\n",
    "    Returns a dataframe of a random sample of the sp500 adj closes over a certain period of time\n",
    "    \"\"\"\n",
    "    sp500_tickers = get_sp500_ticker_list()\n",
    "    sample_tickers = get_sample_ticker(sp500_tickers, s)\n",
    "    full_df = yf.download(sample_tickers, start=start, end=end, interval=interval)\n",
    "    adj_close_df = full_df['Adj Close']\n",
    "    return adj_close_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd8a595f-ff46-41c8-b09c-6d0623473f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_returns(adj_close_df):\n",
    "    df_returns = (adj_close_df.pct_change())*100\n",
    "    df_returns.dropna(axis=0,inplace=True)\n",
    "    return df_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee75c5c7-4f83-49f7-8591-5a9f00df6623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_volatility(returns_df):\n",
    "    realized_vol = returns_df.rolling(5).std()\n",
    "    realized_vol.dropna(inplace=True)\n",
    "    return realized_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f0f4b-bdbb-47c3-a7a8-10acf0c502ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Splitting the dataset by observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7512d4f0-7161-40ad-8497-cd0398cfb63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(df, n=50):\n",
    "    df_test = df.iloc[-n:]\n",
    "    df_train = df.iloc[:-n]\n",
    "    split_date = df.iloc[-n:].index\n",
    "    return df_train, df_test, split_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b886ff03-d137-424a-9aec-a7fe2ec31e20",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55db306a-fda7-4d9f-9312-d07616a678ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def garch(returns, n):\n",
    "\n",
    "    aic_garch = []\n",
    "\n",
    "    for p in range(1, 2): \n",
    "        for q in range(1, 2):\n",
    "            garch = arch_model(returns, mean='zero', vol='GARCH', p=p, q=q)\\\n",
    "                .fit(disp='off') \n",
    "            aic_garch.append(garch.aic) \n",
    "\n",
    "            if garch.aic == np.min(aic_garch): \n",
    "                best_param = (p,q) \n",
    "    \n",
    "    #fitting the best GARCH model\n",
    "    garch = arch_model(returns, mean='zero', vol='GARCH', p=best_param[0], q=best_param[1]).fit(disp='off')\n",
    "\n",
    "    #forecasts\n",
    "    forecasts = garch.forecast(horizon=n, reindex=False)\n",
    "    #forecasts = garch.forecast(horizon=50, start=split_date[0], reindex=True)\n",
    "    return forecasts, forecasts.residual_variance.dropna().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74fcb02f-c800-49a7-9169-db355ed826eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(residuals, realized_vol):\n",
    "    rmse = np.sqrt(mse(realized_vol/100, np.sqrt(residuals/100)))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e0654b-9881-4e37-a5ed-f555044a04de",
   "metadata": {},
   "source": [
    "### ARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c404755-90af-42d3-bf87-d6617e57af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arch(returns, n):\n",
    "    aic_arch = []\n",
    "\n",
    "    for p in range(1, 2): # Iterating ARCH parameter p\n",
    "        arch = arch_model(returns, mean='zero', vol='ARCH', p=p)\\\n",
    "             .fit(disp='off') # Running ARCH(p)\n",
    "        aic_arch.append(arch.aic) # Storing aic for the ARCH(p)\n",
    "\n",
    "        if arch.aic == np.min(aic_arch): \n",
    "             best_param = p # Finding the minimum AIC score\n",
    "                \n",
    "    # Fitting best arch\n",
    "    arch = arch_model(returns, mean='zero', vol='ARCH', p=best_param)\\\n",
    "         .fit(disp='off')\n",
    "    \n",
    "    forecasts = arch.forecast(horizon=n, reindex=False)\n",
    "    \n",
    "    return forecasts, forecasts.residual_variance.dropna().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85028aa3-61b1-4c1c-b630-5fc70f4087af",
   "metadata": {},
   "source": [
    "### GJR garch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fc0c467-5731-4172-bf12-950a36962769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gjr_garch(returns, n):\n",
    "    aic_gjr_garch = []\n",
    "\n",
    "    for p in range(1, 2): \n",
    "        for q in range(1, 2):\n",
    "            gjr_garch = arch_model(returns, mean='zero', vol='GARCH', p=p, o=1, q=q)\\\n",
    "                 .fit(disp='off') \n",
    "            aic_gjr_garch.append(gjr_garch.aic) \n",
    "\n",
    "            if gjr_garch.aic == np.min(aic_gjr_garch): \n",
    "                 best_param = p, q # Finding the minimum AIC score\n",
    "    \n",
    "    gjr_garch = arch_model(returns, mean='zero', vol='ARCH', p=best_param[0], o=1,\n",
    "                       q=best_param[1]).fit(disp='off')\n",
    "    \n",
    "    forecasts = gjr_garch.forecast(horizon=n, reindex=True)\n",
    "    \n",
    "    return forecasts, forecasts.residual_variance.dropna().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42224fd5-729f-4f6f-83c4-93817b55b844",
   "metadata": {},
   "source": [
    "### EGARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75145c60-8213-4f60-acc2-ed11298e0998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def egarch(returns, n):\n",
    "    aic_egarch = []\n",
    "\n",
    "    for p in range(1, 2):\n",
    "        for q in range(1, 2):\n",
    "            egarch = arch_model(returns, mean='zero', vol='EGARCH', p=p, q=q)\\\n",
    "                  .fit(disp='off')\n",
    "            aic_egarch.append(egarch.aic)\n",
    "            if egarch.aic == np.min(aic_egarch):\n",
    "                best_param = (p, q)\n",
    "    \n",
    "    egarch = arch_model(returns, mean='zero', vol='EGARCH',\n",
    "                        p=best_param[0], q=best_param[1], dist=\"skewt\").fit(disp='off')\n",
    "    \n",
    "    forecasts = egarch.forecast(horizon=50, method='simulation', reindex=False)\n",
    "    \n",
    "    return forecasts, forecasts.residual_variance.dropna().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcaeb47-52a5-4828-94cf-1f4c025faf2e",
   "metadata": {},
   "source": [
    "### Neural nets, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3af32376-5ab2-4199-bbf2-14a883f8ee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_volatility(volatility_df):\n",
    "    realized_vol = {}\n",
    "    for ticker in volatility_df.columns:\n",
    "        realized_vol[ticker] = pd.DataFrame(volatility_df[ticker]).reset_index(drop=True)\n",
    "    return realized_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f6f3202-7ac9-4c62-a4e3-5c4310b04a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_returns(returns_df):\n",
    "    returns_svm = {}\n",
    "    for ticker in returns_df.columns:\n",
    "        returns_svm[ticker] = returns_df[ticker]**2\n",
    "        returns_svm[ticker] = returns_svm[ticker].reset_index()\n",
    "        del returns_svm[ticker]['Date']\n",
    "    return returns_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8a07ca1-158e-476d-bf0c-cd26ee4c6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_ret_vol(returns_df, volatility_df):\n",
    "    Xs = {}\n",
    "    realized_vol = get_svm_volatility(volatility_df)#[:-4]\n",
    "    returns_svm = get_svm_returns(returns_df)\n",
    "    for ticker in volatility_df.keys():\n",
    "        Xs[ticker] = pd.concat([realized_vol[ticker], returns_svm[ticker][:-4]], axis=1, ignore_index=True)\n",
    "        \n",
    "    return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13411b68-30e5-4b85-8164-439b5e6241aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_train_test_svm(Xs, n=50):\n",
    "#     Xs_splitted = {}\n",
    "#     for ticker, X in Xs.items():\n",
    "#         Xs_splitted[ticker] = {}\n",
    "#         Xs_splitted[ticker]['train'] = X.iloc[:-n]\n",
    "#         Xs_splitted[ticker]['test'] = X.iloc[-n:]\n",
    "#     return Xs_splitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111b849e-65f4-41ca-8987-f73b337ea51b",
   "metadata": {},
   "source": [
    "<!-- Xs_splitted = get_train_test_svm(Xs, n=50) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7481f1f9-8eb9-443a-bda2-5b4ec3854032",
   "metadata": {},
   "source": [
    "### Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93722739-fc91-40ad-a5d7-4c91d363c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(Xs, n, NN_vol, para_grid_NN):\n",
    "    X_predictions = {}\n",
    "    for ticker, X in Xs.items():\n",
    "        realized_vol = X[0]\n",
    "        clf = RandomizedSearchCV(NN_vol, para_grid_NN)\n",
    "        clf.fit(X.iloc[:-n].values,\n",
    "            realized_vol.iloc[1:-(n-1)].values.reshape(-1, ))\n",
    "        X_predictions[ticker] = clf.predict(X.iloc[-n:])\n",
    "    return X_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b6cf6b1-19ce-40ef-9f95-34ccb7101376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmses(s, n, start, end, interval):\n",
    "    \"\"\"\n",
    "    s = how many companies\n",
    "    n = sample size for testing\n",
    "    \"\"\"\n",
    "    NN_vol = MLPRegressor(learning_rate_init=0.001, random_state=1)\n",
    "    para_grid_NN = {'hidden_layer_sizes': [(100, 50), (50, 50), (10, 100)],\n",
    "                'max_iter': [500, 1000],\n",
    "                'alpha': [0.00005, 0.0005 ]}\n",
    "    rmses = {}\n",
    "    \n",
    "    # Get data\n",
    "    adj_close_df = get_adj_close_df(s=s, start=start, end=end, interval=interval)\n",
    "    \n",
    "    # Get returns and volatility for arch type models\n",
    "    returns_df = get_returns(adj_close_df)\n",
    "    volatility_df = get_volatility(returns_df)\n",
    "    \n",
    "    # Splitting sets for arch type models\n",
    "    df_train_vol, df_test_vol, split_date = split_df(volatility_df, n)\n",
    "    df_train_ret, df_test_ret, split_date = split_df(returns_df, n)\n",
    "    \n",
    "    # Neural net preprocessing\n",
    "    Xs = concat_ret_vol(returns_df, volatility_df)\n",
    "    nn_predictions = neural_net(Xs, n, NN_vol, para_grid_NN)\n",
    "    \n",
    "    # Getting rmses\n",
    "    for ticker in df_test_vol.columns:\n",
    "        rmses[ticker] = {}\n",
    "        \n",
    "        #Neural net\n",
    "        rmses[ticker]['nn'] = np.sqrt(mse(df_test_vol[ticker] / 100, nn_predictions[ticker] / 100))\n",
    "        \n",
    "        #Garch\n",
    "        forecasts, residuals = garch(df_train_ret[ticker], n)\n",
    "        rmses[ticker]['garch'] = get_rmse(residuals, df_test_vol[ticker])\n",
    "        \n",
    "        #Arch\n",
    "        forecasts, residuals = garch(df_train_ret[ticker], n)\n",
    "        rmses[ticker]['arch'] = get_rmse(residuals, df_test_vol[ticker])\n",
    "        \n",
    "        #GJR-garch\n",
    "        forecasts, residuals = gjr_garch(df_train_ret[ticker], n)\n",
    "        rmses[ticker]['gjr_garch'] = get_rmse(residuals, df_test_vol[ticker])\n",
    "        \n",
    "        #Egarch\n",
    "        forecasts, residuals = egarch(df_train_ret[ticker], n)\n",
    "        rmses[ticker]['egarch'] = get_rmse(residuals, df_test_vol[ticker])\n",
    "        \n",
    "    return rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b26a673-6ab6-4097-9f17-10221c1b1aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2 of 2 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EIX': {'nn': 0.0015917607444057792,\n",
       "  'garch': 0.16940785327105573,\n",
       "  'arch': 0.16940785327105573,\n",
       "  'gjr_garch': 0.26539319582333015,\n",
       "  'egarch': 0.16542928634009763},\n",
       " 'TECH': {'nn': 0.0017618021597707815,\n",
       "  'garch': 0.22191049103699945,\n",
       "  'arch': 0.22191049103699945,\n",
       "  'gjr_garch': 0.2145386468196369,\n",
       "  'egarch': 0.19099953804906397}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rmses(s=2, n=50, start='2020-01-01', end='2021-12-31', interval='1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477bce23-0fd1-4350-8679-ee42e5aa7e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_daily_1y = get_rmses(s=5, n=30, start='2016-03-17', end='2017-03-17', interval='1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649e410-ceca-419c-acc5-afe9f85f7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dailyget_rmses(s=5, n=30, start='2016-03-17', end='2021-11-25', interval='1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c12e590-8576-4d44-a3a5-10855b6eac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rmses(s=5, n=50, start='2021-06-21', end='2021-06-25', interval='15m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b4136-9eb2-4e16-a18b-8ef247b0d2df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
